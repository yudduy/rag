# Enhanced RAG System with SOTA Features

A production-ready Retrieval-Augmented Generation system built on [LlamaIndex](https://www.llamaindex.ai/) with advanced capabilities including agentic workflows, semantic caching, hallucination detection, and multimodal support.

## âœ¨ Features

- ğŸ¤– **Agentic Workflows** - Intelligent query routing and decomposition
- âš¡ **Semantic Caching** - Redis-based caching with 31%+ hit rates
- ğŸ›¡ï¸ **Hallucination Detection** - GPT-4o-mini verification and confidence scoring
- ğŸ–¼ï¸ **Multimodal Support** - CLIP integration for text-image search
- ğŸ¯ **Performance Profiles** - Configurable quality/cost/speed optimization
- ğŸ“Š **Production Ready** - Comprehensive monitoring and error handling

## ğŸš€ Quick Start

### Prerequisites

1. **Install UV Package Manager**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

2. **Get OpenAI API Key** from [OpenAI Platform](https://platform.openai.com/api-keys)

3. **Redis (Optional but Recommended)**
   ```bash
   # macOS
   brew install redis && brew services start redis
   
   # Ubuntu/Debian
   sudo apt install redis-server && sudo systemctl start redis-server
   
   # Docker
   docker run -d -p 6379:6379 redis:alpine
   ```

### Installation & Setup

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd rag
   uv sync
   ```

2. **Configure Environment**
   ```bash
   # Create environment file
   cp src/.env.example src/.env
   # Edit src/.env with your OpenAI API key:
   OPENAI_API_KEY=your_api_key_here
   ```

3. **Add Documents and Generate Index**
   ```bash
   # Place your documents in ui/data/
   cp your_documents.pdf ui/data/
   
   # Generate the index
   uv run generate
   ```

### Deploy & Run

1. **Start API Server** (Terminal 1)
   ```bash
   uv run -m llama_deploy.apiserver
   ```

2. **Deploy Workflow** (Terminal 2)
   ```bash
   uv run llamactl deploy llama_deploy.yml
   ```

3. **Access the UI**
   - Web Interface: http://localhost:4501/deployments/chat/ui
   - API Documentation: http://localhost:4501/docs

## ğŸ”§ Configuration

### Environment Variables

Create `src/.env` with:

```bash
# Required
OPENAI_API_KEY=your_openai_api_key_here

# Optional - Enhanced Features
SEMANTIC_CACHE_ENABLED=true
VERIFICATION_ENABLED=true
AGENT_ROUTING_ENABLED=true
PERFORMANCE_PROFILE=balanced  # high_accuracy|balanced|cost_optimized|speed

# Redis (for caching)
REDIS_CACHE_URL=redis://localhost:6379

# Multimodal (requires additional dependencies)
MULTIMODAL_ENABLED=false
```

### Performance Profiles

Choose your optimization strategy:

- **`high_accuracy`** - 96% quality, optimized for precision
- **`balanced`** - 90% quality with balanced cost/latency
- **`cost_optimized`** - 85% quality, minimized costs
- **`speed`** - 80% quality, sub-second responses

## ğŸ”Œ API Usage

### Basic Query
```bash
curl -X POST 'http://localhost:4501/deployments/chat/tasks/create' \
  -H 'Content-Type: application/json' \
  -d '{"input": "{\"user_msg\":\"What is machine learning?\"}", "service_id": "workflow"}'
```

### With Performance Profile
```bash
curl -X POST 'http://localhost:4501/deployments/chat/tasks/create' \
  -d '{"input": "{\"user_msg\":\"Complex query\",\"profile\":\"high_accuracy\"}", "service_id": "workflow"}'
```

### Monitor System Health
```bash
curl 'http://localhost:4501/health'
curl 'http://localhost:4501/metrics'
```

## ğŸ¨ UI Development

### Frontend Development
```bash
cd ui
npm install
npm run dev  # Development server
npm run build  # Production build
```

### UI Features
- Real-time chat interface
- Citation display with source links  
- Performance metrics and cache hit indicators
- Multimodal content display
- Text-to-speech integration
- Configurable performance profiles

## ğŸš€ Advanced Features

### Multimodal Support
Enable image processing and cross-modal search:
```bash
# Install additional dependencies
uv add clip-by-openai pillow opencv-python pyttsx3 gTTS pdf2image pytesseract

# Enable in environment
MULTIMODAL_ENABLED=true
IMAGE_INDEXING_ENABLED=true
```

### Semantic Caching
Reduce costs with intelligent caching:
```bash
# Ensure Redis is running
redis-cli ping  # Should return "PONG"

# Enable in environment  
SEMANTIC_CACHE_ENABLED=true
CACHE_SIMILARITY_THRESHOLD=0.97
```

### Hallucination Detection
Add response verification:
```bash
VERIFICATION_ENABLED=true
VERIFICATION_THRESHOLD=0.8
MULTI_LEVEL_CONFIDENCE=true
```

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI        â”‚    â”‚  LlamaDeploy     â”‚    â”‚  Enhanced RAG   â”‚
â”‚  (TypeScript)   â”‚ -> â”‚   API Server     â”‚ -> â”‚   Workflow      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚  Semantic Cache â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚    (Redis)      â”‚              â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚  Vector Store   â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚   (Persistent)  â”‚              â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚ Verification    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ (GPT-4o-mini)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Key Components

- **`src/unified_workflow.py`** - Main orchestrator with intelligent routing
- **`src/workflow.py`** - Base workflow implementation
- **`src/cache.py`** - Redis semantic caching system
- **`src/verification.py`** - Hallucination detection and confidence scoring
- **`src/multimodal.py`** - CLIP integration for image-text processing
- **`ui/index.ts`** - TypeScript UI server

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes with tests
4. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

- ğŸ› [GitHub Issues](https://github.com/your-repo/issues) for bug reports
- ğŸ’¬ [GitHub Discussions](https://github.com/your-repo/discussions) for questions
- ğŸ“– Check the `/docs` endpoints for API documentation

---

**Built with â¤ï¸ using [LlamaIndex](https://docs.llamaindex.ai) and [LlamaDeploy](https://github.com/run-llama/llama_deploy)**